@Article{Mueller2019,
  author        = {Rafael MÃ¼ller and Simon Kornblith and Geoffrey Hinton},
  title         = {When Does Label Smoothing Help?},
  year          = {2019},
  abstract      = {The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions.},
  archiveprefix = {arXiv},
  eprint        = {1906.02629},
  file          = {:Mueller2019 - When Does Label Smoothing Help_.pdf:PDF},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Szegedy2015,
  author        = {Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jonathon Shlens and Zbigniew Wojna},
  title         = {Rethinking the Inception Architecture for Computer Vision},
  year          = {2015},
  abstract      = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.},
  archiveprefix = {arXiv},
  eprint        = {1512.00567},
  file          = {:Szegedy2015 - Rethinking the Inception Architecture for Computer Vision.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
  ranking       = {rank5},
  relevance     = {relevant},
}

@Article{Erickson2020,
  author        = {Nick Erickson and Jonas Mueller and Alexander Shirkov and Hang Zhang and Pedro Larroy and Mu Li and Alexander Smola},
  title         = {AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data},
  year          = {2020},
  month         = mar,
  abstract      = {We introduce AutoGluon-Tabular, an open-source AutoML framework that requires only a single line of Python to train highly accurate machine learning models on an unprocessed tabular dataset such as a CSV file. Unlike existing AutoML frameworks that primarily focus on model/hyperparameter selection, AutoGluon-Tabular succeeds by ensembling multiple models and stacking them in multiple layers. Experiments reveal that our multi-layer combination of many models offers better use of allocated training time than seeking out the best. A second contribution is an extensive evaluation of public and commercial AutoML platforms including TPOT, H2O, AutoWEKA, auto-sklearn, AutoGluon, and Google AutoML Tables. Tests on a suite of 50 classification and regression tasks from Kaggle and the OpenML AutoML Benchmark reveal that AutoGluon is faster, more robust, and much more accurate. We find that AutoGluon often even outperforms the best-in-hindsight combination of all of its competitors. In two popular Kaggle competitions, AutoGluon beat 99% of the participating data scientists after merely 4h of training on the raw data.},
  archiveprefix = {arXiv},
  eprint        = {2003.06505},
  file          = {:Erickson2020 - AutoGluon Tabular_ Robust and Accurate AutoML for Structured Data.pdf:PDF},
  keywords      = {stat.ML, cs.LG},
  primaryclass  = {stat.ML},
}

@Article{Yang2021,
  author         = {Yuzhe Yang and Kaiwen Zha and Ying-Cong Chen and Hao Wang and Dina Katabi},
  title          = {Delving into Deep Imbalanced Regression},
  year           = {2021},
  month          = feb,
  abstract       = {Real-world data often exhibit imbalanced distributions, where certain target values have significantly fewer observations. Existing techniques for dealing with imbalanced data focus on targets with categorical indices, i.e., different classes. However, many tasks involve continuous targets, where hard boundaries between classes do not exist. We define Deep Imbalanced Regression (DIR) as learning from such imbalanced data with continuous targets, dealing with potential missing data for certain target values, and generalizing to the entire target range. Motivated by the intrinsic difference between categorical and continuous label space, we propose distribution smoothing for both labels and features, which explicitly acknowledges the effects of nearby targets, and calibrates both label and learned feature distributions. We curate and benchmark large-scale DIR datasets from common real-world tasks in computer vision, natural language processing, and healthcare domains. Extensive experiments verify the superior performance of our strategies. Our work fills the gap in benchmarks and techniques for practical imbalanced regression problems. Code and data are available at https://github.com/YyzHarry/imbalanced-regression.},
  archiveprefix  = {arXiv},
  eprint         = {2102.09554},
  file           = {:Yang2021 - Delving into Deep Imbalanced Regression.pdf:PDF},
  keywords       = {cs.LG, cs.AI, cs.CV},
  primaryclass   = {cs.LG},
  qualityassured = {qualityAssured},
}

@Article{Blundell2015,
  author        = {Charles Blundell and Julien Cornebise and Koray Kavukcuoglu and Daan Wierstra},
  title         = {Weight Uncertainty in Neural Networks},
  year          = {2015},
  month         = may,
  abstract      = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
  archiveprefix = {arXiv},
  eprint        = {1505.05424},
  file          = {:Blundell2015 - Weight Uncertainty in Neural Networks.pdf:PDF},
  keywords      = {stat.ML, cs.LG},
  primaryclass  = {stat.ML},
}

@Article{Barron2017,
  author        = {Jonathan T. Barron},
  title         = {A General and Adaptive Robust Loss Function},
  year          = {2017},
  month         = jan,
  abstract      = {We present a generalization of the Cauchy/Lorentzian, Geman-McClure, Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2 loss functions. By introducing robustness as a continuous parameter, our loss function allows algorithms built around robust loss minimization to be generalized, which improves performance on basic vision tasks such as registration and clustering. Interpreting our loss as the negative log of a univariate density yields a general probability distribution that includes normal and Cauchy distributions as special cases. This probabilistic interpretation enables the training of neural networks in which the robustness of the loss automatically adapts itself during training, which improves performance on learning-based tasks such as generative image synthesis and unsupervised monocular depth estimation, without requiring any manual parameter tuning.},
  archiveprefix = {arXiv},
  eprint        = {1701.03077},
  file          = {:Barron2017 - A General and Adaptive Robust Loss Function.pdf:PDF},
  keywords      = {cs.CV, cs.LG, stat.ML},
  primaryclass  = {cs.CV},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: fileDirectory:/home/zgcarvalho/Documentos/my_refs;}
